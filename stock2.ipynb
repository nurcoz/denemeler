{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "============================================================================\n",
        "FULL LEAKAGE MODE - TÃœM SIZILAR AÃ‡IK!\n",
        "============================================================================\n",
        "Hipotez: Makale ÅŸu hatalarÄ± yapmÄ±ÅŸ olabilir:\n",
        "1. âŒ Normalization BEFORE split\n",
        "2. âŒ NO LAG (same-day features)\n",
        "3. âŒ RANDOM shuffle (future data in train)\n",
        "4. âŒ Look-ahead bias (indicators already contain target info)\n",
        "\n",
        "Test: TÃ¼m bunlarÄ± yapalÄ±m ve %90 accuracy elde edelim!\n",
        "============================================================================\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "print(\"ðŸ“¦ YÃ¼kleniyor...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "                      \"yfinance\", \"ta\", \"scikit-learn\", \"pandas\", \"numpy\"])\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ta\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… HazÄ±r!\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# VERÄ°\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"VERÄ° Ã‡EKME - KOSPI\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "ticker = '^KS11'\n",
        "data = yf.download(ticker, start=\"2011-01-01\", end=\"2020-09-27\",\n",
        "                  progress=False, auto_adjust=True)\n",
        "\n",
        "if isinstance(data.columns, pd.MultiIndex):\n",
        "    data.columns = data.columns.get_level_values(0)\n",
        "\n",
        "data = data[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
        "data = data.dropna()\n",
        "print(f\"âœ… {len(data)} gÃ¼n\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# TEKNÄ°K GÃ–STERGELER\n",
        "# ============================================================================\n",
        "\n",
        "def calculate_indicators(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    high = df['High'].squeeze()\n",
        "    low = df['Low'].squeeze()\n",
        "    close = df['Close'].squeeze()\n",
        "\n",
        "    # Stochastic\n",
        "    stoch = ta.momentum.StochasticOscillator(high, low, close, window=14, smooth_window=3)\n",
        "    df['Stochastic_K'] = stoch.stoch()\n",
        "    df['Stochastic_D'] = stoch.stoch_signal()\n",
        "\n",
        "    # ROC\n",
        "    df['ROC'] = ta.momentum.ROCIndicator(close, window=10).roc()\n",
        "\n",
        "    # Williams %R\n",
        "    df['Williams_R'] = ta.momentum.WilliamsRIndicator(high, low, close, lbp=14).williams_r()\n",
        "\n",
        "    # Momentum\n",
        "    df['Momentum'] = close.diff(4)\n",
        "\n",
        "    # Disparity\n",
        "    ma5 = close.rolling(5).mean()\n",
        "    ma14 = close.rolling(14).mean()\n",
        "    df['Disparity_5'] = np.where(ma5 != 0, (close / ma5) * 100, 100)\n",
        "    df['Disparity_14'] = np.where(ma14 != 0, (close / ma14) * 100, 100)\n",
        "\n",
        "    # OSCP\n",
        "    ma10 = close.rolling(10).mean()\n",
        "    df['OSCP'] = np.where(ma5 != 0, ((ma5 - ma10) / ma5), 0)\n",
        "\n",
        "    # CCI\n",
        "    df['CCI'] = ta.trend.CCIIndicator(high, low, close, window=20).cci()\n",
        "\n",
        "    # RSI\n",
        "    df['RSI'] = ta.momentum.RSIIndicator(close, window=14).rsi()\n",
        "\n",
        "    # Pivot Points\n",
        "    prev_high = high.shift(1)\n",
        "    prev_low = low.shift(1)\n",
        "    prev_close = close.shift(1)\n",
        "\n",
        "    df['Pivot_Point'] = (prev_high + prev_low + prev_close) / 3\n",
        "    df['S1'] = (df['Pivot_Point'] * 2) - prev_high\n",
        "    df['S2'] = df['Pivot_Point'] - (prev_high - prev_low)\n",
        "    df['R1'] = (df['Pivot_Point'] * 2) - prev_low\n",
        "    df['R2'] = df['Pivot_Point'] + (prev_high - prev_low)\n",
        "\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    return df\n",
        "\n",
        "data = calculate_indicators(data)\n",
        "print(\"âœ… GÃ¶stergeler hesaplandÄ±\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# EXTREME LEAKAGE SCENARIOS\n",
        "# ============================================================================\n",
        "\n",
        "def scenario_1_worst_leakage(df):\n",
        "    \"\"\"\n",
        "    âŒâŒâŒ EN KÃ–TÃœ SENARYO - TÃœM LEAKAGE'LAR AÃ‡IK\n",
        "\n",
        "    1. NO LAG (bugÃ¼nÃ¼n gÃ¶stergeleri)\n",
        "    2. Normalize BEFORE split (tÃ¼m veriye fit)\n",
        "    3. RANDOM shuffle (gelecek train'de)\n",
        "    4. Same-day target (bugÃ¼nkÃ¼ yÃ¶n)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    features = ['Stochastic_K', 'Stochastic_D', 'ROC', 'Williams_R',\n",
        "                'Momentum', 'Disparity_5', 'Disparity_14', 'OSCP',\n",
        "                'CCI', 'RSI', 'Pivot_Point', 'S1', 'S2', 'R1', 'R2']\n",
        "\n",
        "    # âŒ 1. SAME-DAY TARGET (bugÃ¼nÃ¼n kapanÄ±ÅŸ yÃ¶nÃ¼)\n",
        "    df['Target'] = (df['Close'] > df['Close'].shift(1)).astype(int)\n",
        "\n",
        "    df = df.dropna(subset=features + ['Target'])\n",
        "\n",
        "    # âŒ 2. NORMALIZE ALL DATA FIRST (leakage!)\n",
        "    scaler = MinMaxScaler()\n",
        "    df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "    X = df[features].values\n",
        "    y = df['Target'].values\n",
        "\n",
        "    # âŒ 3. RANDOM SHUFFLE SPLIT (gelecek verisi gÃ¶rÃ¼lÃ¼yor!)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, shuffle=True, stratify=y\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, \"EXTREME LEAKAGE (NO LAG + SAME DAY)\"\n",
        "\n",
        "\n",
        "def scenario_2_normalize_before_split(df):\n",
        "    \"\"\"\n",
        "    âŒâŒ Normalize BEFORE split + Random shuffle\n",
        "\n",
        "    1. LAG VAR (t-1 features) âœ…\n",
        "    2. Normalize BEFORE split âŒ\n",
        "    3. RANDOM shuffle âŒ\n",
        "    4. Next-day target âœ…\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    features = ['Stochastic_K', 'Stochastic_D', 'ROC', 'Williams_R',\n",
        "                'Momentum', 'Disparity_5', 'Disparity_14', 'OSCP',\n",
        "                'CCI', 'RSI', 'Pivot_Point', 'S1', 'S2', 'R1', 'R2']\n",
        "\n",
        "    # Target (next day)\n",
        "    df['Target'] = (df['Close'].shift(-1) > df['Close']).astype(int)\n",
        "    df = df.iloc[:-1]\n",
        "\n",
        "    df = df.dropna(subset=features + ['Target'])\n",
        "\n",
        "    # âŒ NORMALIZE FIRST (leakage!)\n",
        "    scaler = MinMaxScaler()\n",
        "    df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "    # Lag apply\n",
        "    lagged_features = []\n",
        "    for feat in features:\n",
        "        lagged_col = f'{feat}_lag1'\n",
        "        df[lagged_col] = df[feat].shift(1)\n",
        "        lagged_features.append(lagged_col)\n",
        "\n",
        "    df = df.dropna(subset=lagged_features)\n",
        "\n",
        "    X = df[lagged_features].values\n",
        "    y = df['Target'].values\n",
        "\n",
        "    # âŒ RANDOM SHUFFLE\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, shuffle=True, stratify=y\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, \"NORMALIZE BEFORE SPLIT + SHUFFLE\"\n",
        "\n",
        "\n",
        "def scenario_3_random_cv_only(df):\n",
        "    \"\"\"\n",
        "    âŒ Random CV (StratifiedKFold shuffle=True)\n",
        "\n",
        "    1. LAG VAR âœ…\n",
        "    2. Normalize correctly âœ…\n",
        "    3. Temporal split âœ…\n",
        "    4. BUT: Random CV folds âŒ\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    features = ['Stochastic_K', 'Stochastic_D', 'ROC', 'Williams_R',\n",
        "                'Momentum', 'Disparity_5', 'Disparity_14', 'OSCP',\n",
        "                'CCI', 'RSI', 'Pivot_Point', 'S1', 'S2', 'R1', 'R2']\n",
        "\n",
        "    # Target\n",
        "    df['Target'] = (df['Close'].shift(-1) > df['Close']).astype(int)\n",
        "    df = df.iloc[:-1]\n",
        "\n",
        "    df = df.dropna(subset=features + ['Target'])\n",
        "\n",
        "    # Lag\n",
        "    lagged_features = []\n",
        "    for feat in features:\n",
        "        lagged_col = f'{feat}_lag1'\n",
        "        df[lagged_col] = df[feat].shift(1)\n",
        "        lagged_features.append(lagged_col)\n",
        "\n",
        "    df = df.dropna(subset=lagged_features)\n",
        "\n",
        "    X = df[lagged_features].copy()\n",
        "    y = df['Target'].copy()\n",
        "\n",
        "    # Temporal split\n",
        "    n_train = int(len(X) * 0.8)\n",
        "    X_train = X.iloc[:n_train]\n",
        "    X_test = X.iloc[n_train:]\n",
        "    y_train = y.iloc[:n_train].values\n",
        "    y_test = y.iloc[n_train:].values\n",
        "\n",
        "    # Normalize correctly\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # âŒ BUT use random CV for hyperparameter tuning\n",
        "    # (This will be shown in cross-validation score)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, \"CORRECT BUT RANDOM CV\"\n",
        "\n",
        "\n",
        "def scenario_4_correct(df):\n",
        "    \"\"\"\n",
        "    âœ… CORRECT METHOD\n",
        "\n",
        "    Everything done properly\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    features = ['Stochastic_K', 'Stochastic_D', 'ROC', 'Williams_R',\n",
        "                'Momentum', 'Disparity_5', 'Disparity_14', 'OSCP',\n",
        "                'CCI', 'RSI', 'Pivot_Point', 'S1', 'S2', 'R1', 'R2']\n",
        "\n",
        "    # Target\n",
        "    df['Target'] = (df['Close'].shift(-1) > df['Close']).astype(int)\n",
        "    df = df.iloc[:-1]\n",
        "\n",
        "    df = df.dropna(subset=features + ['Target'])\n",
        "\n",
        "    # Lag\n",
        "    lagged_features = []\n",
        "    for feat in features:\n",
        "        lagged_col = f'{feat}_lag1'\n",
        "        df[lagged_col] = df[feat].shift(1)\n",
        "        lagged_features.append(lagged_col)\n",
        "\n",
        "    df = df.dropna(subset=lagged_features)\n",
        "\n",
        "    X = df[lagged_features].copy()\n",
        "    y = df['Target'].copy()\n",
        "\n",
        "    # Temporal split\n",
        "    n_train = int(len(X) * 0.8)\n",
        "    X_train = X.iloc[:n_train]\n",
        "    X_test = X.iloc[n_train:]\n",
        "    y_train = y.iloc[:n_train].values\n",
        "    y_test = y.iloc[n_train:].values\n",
        "\n",
        "    # Normalize correctly\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, \"âœ… CORRECT METHOD\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_scenario(X_train, X_test, y_train, y_test, name):\n",
        "    \"\"\"Train and evaluate\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")\n",
        "    print(f\"Class dist: UP={y_train.mean()*100:.1f}%\")\n",
        "\n",
        "    # Simple SVM (paper's parameters)\n",
        "    svm = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "\n",
        "    print(\"\\nTraining SVM...\")\n",
        "    svm.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = svm.predict(X_test)\n",
        "\n",
        "    # Metrics\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\n{'RESULTS':^80}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Test Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(f\"                Predicted DOWN  Predicted UP\")\n",
        "    print(f\"Actual DOWN          {cm[0,0]:<8}      {cm[0,1]:<8}\")\n",
        "    print(f\"Actual UP            {cm[1,0]:<8}      {cm[1,1]:<8}\")\n",
        "\n",
        "    # Class-wise\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    down_acc = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    up_acc = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "    print(f\"\\nClass-wise Accuracy:\")\n",
        "    print(f\"DOWN: {down_acc:.4f} ({down_acc*100:.1f}%)\")\n",
        "    print(f\"UP:   {up_acc:.4f} ({up_acc*100:.1f}%)\")\n",
        "    print(f\"Balance: {abs(down_acc - up_acc):.4f}\")\n",
        "\n",
        "    # Verdict\n",
        "    if acc >= 0.85:\n",
        "        print(f\"\\nðŸŽ‰ PAPER ACCURACY ACHIEVED! ({acc*100:.1f}%)\")\n",
        "    elif acc >= 0.70:\n",
        "        print(f\"\\nðŸŸ¡ HIGH ACCURACY ({acc*100:.1f}%) - Likely data leakage\")\n",
        "    elif acc >= 0.60:\n",
        "        print(f\"\\nðŸŸ¢ GOOD ACCURACY ({acc*100:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"\\nðŸ”µ REALISTIC ACCURACY ({acc*100:.1f}%) - No leakage\")\n",
        "\n",
        "    return acc\n",
        "\n",
        "# ============================================================================\n",
        "# RUN ALL SCENARIOS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TESTING ALL LEAKAGE SCENARIOS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Scenario 1: EXTREME LEAKAGE\n",
        "X_train, X_test, y_train, y_test, name = scenario_1_worst_leakage(data)\n",
        "results['Scenario 1'] = evaluate_scenario(X_train, X_test, y_train, y_test, name)\n",
        "\n",
        "# Scenario 2: Normalize before split\n",
        "X_train, X_test, y_train, y_test, name = scenario_2_normalize_before_split(data)\n",
        "results['Scenario 2'] = evaluate_scenario(X_train, X_test, y_train, y_test, name)\n",
        "\n",
        "# Scenario 3: Random CV\n",
        "X_train, X_test, y_train, y_test, name = scenario_3_random_cv_only(data)\n",
        "results['Scenario 3'] = evaluate_scenario(X_train, X_test, y_train, y_test, name)\n",
        "\n",
        "# Scenario 4: CORRECT\n",
        "X_train, X_test, y_train, y_test, name = scenario_4_correct(data)\n",
        "results['Scenario 4'] = evaluate_scenario(X_train, X_test, y_train, y_test, name)\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“Š SUMMARY - ACCURACY COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n{'Scenario':<45} {'Accuracy':<12} {'Status'}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "scenarios = [\n",
        "    ('Scenario 1: EXTREME LEAKAGE (NO LAG + SAME DAY)', results['Scenario 1']),\n",
        "    ('Scenario 2: NORMALIZE BEFORE SPLIT + SHUFFLE', results['Scenario 2']),\n",
        "    ('Scenario 3: CORRECT BUT RANDOM CV', results['Scenario 3']),\n",
        "    ('Scenario 4: âœ… FULLY CORRECT', results['Scenario 4'])\n",
        "]\n",
        "\n",
        "for name, acc in scenarios:\n",
        "    if acc >= 0.85:\n",
        "        status = \"ðŸŽ‰ PAPER LEVEL\"\n",
        "    elif acc >= 0.70:\n",
        "        status = \"ðŸŸ¡ HIGH (Leakage)\"\n",
        "    elif acc >= 0.60:\n",
        "        status = \"ðŸŸ¢ GOOD\"\n",
        "    else:\n",
        "        status = \"ðŸ”µ REALISTIC\"\n",
        "\n",
        "    print(f\"{name:<45} {acc*100:>5.2f}%       {status}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ’¡ CONCLUSIONS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "1. If Scenario 1-2 achieves 85-90%, the paper has SEVERE data leakage\n",
        "2. If Scenario 3 is high but 4 is low, paper used random CV incorrectly\n",
        "3. If ALL scenarios show ~55-60%, your implementation is CORRECT\n",
        "   and the paper's methodology is QUESTIONABLE\n",
        "\n",
        "Your realistic accuracy (~56%) is NORMAL for financial prediction!\n",
        "Papers reporting 85-90% are almost always using incorrect methodology.\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03ho6Q4edPvs",
        "outputId": "49b47ffd-82d9-447d-fd0a-39ac2ea8c4f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¦ YÃ¼kleniyor...\n",
            "âœ… HazÄ±r!\n",
            "\n",
            "================================================================================\n",
            "VERÄ° Ã‡EKME - KOSPI\n",
            "================================================================================\n",
            "âœ… 2397 gÃ¼n\n",
            "\n",
            "âœ… GÃ¶stergeler hesaplandÄ±\n",
            "\n",
            "\n",
            "================================================================================\n",
            "TESTING ALL LEAKAGE SCENARIOS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "EXTREME LEAKAGE (NO LAG + SAME DAY)\n",
            "================================================================================\n",
            "Train: 1902 | Test: 476\n",
            "Class dist: UP=52.3%\n",
            "\n",
            "Training SVM...\n",
            "\n",
            "                                    RESULTS                                     \n",
            "--------------------------------------------------------------------------------\n",
            "Test Accuracy: 0.7878 (78.78%)\n",
            "\n",
            "Confusion Matrix:\n",
            "                Predicted DOWN  Predicted UP\n",
            "Actual DOWN          163           64      \n",
            "Actual UP            37            212     \n",
            "\n",
            "Class-wise Accuracy:\n",
            "DOWN: 0.7181 (71.8%)\n",
            "UP:   0.8514 (85.1%)\n",
            "Balance: 0.1333\n",
            "\n",
            "ðŸŸ¡ HIGH ACCURACY (78.8%) - Likely data leakage\n",
            "\n",
            "================================================================================\n",
            "NORMALIZE BEFORE SPLIT + SHUFFLE\n",
            "================================================================================\n",
            "Train: 1900 | Test: 476\n",
            "Class dist: UP=52.4%\n",
            "\n",
            "Training SVM...\n",
            "\n",
            "                                    RESULTS                                     \n",
            "--------------------------------------------------------------------------------\n",
            "Test Accuracy: 0.5231 (52.31%)\n",
            "\n",
            "Confusion Matrix:\n",
            "                Predicted DOWN  Predicted UP\n",
            "Actual DOWN          0             227     \n",
            "Actual UP            0             249     \n",
            "\n",
            "Class-wise Accuracy:\n",
            "DOWN: 0.0000 (0.0%)\n",
            "UP:   1.0000 (100.0%)\n",
            "Balance: 1.0000\n",
            "\n",
            "ðŸ”µ REALISTIC ACCURACY (52.3%) - No leakage\n",
            "\n",
            "================================================================================\n",
            "CORRECT BUT RANDOM CV\n",
            "================================================================================\n",
            "Train: 1900 | Test: 476\n",
            "Class dist: UP=51.4%\n",
            "\n",
            "Training SVM...\n",
            "\n",
            "                                    RESULTS                                     \n",
            "--------------------------------------------------------------------------------\n",
            "Test Accuracy: 0.5630 (56.30%)\n",
            "\n",
            "Confusion Matrix:\n",
            "                Predicted DOWN  Predicted UP\n",
            "Actual DOWN          0             208     \n",
            "Actual UP            0             268     \n",
            "\n",
            "Class-wise Accuracy:\n",
            "DOWN: 0.0000 (0.0%)\n",
            "UP:   1.0000 (100.0%)\n",
            "Balance: 1.0000\n",
            "\n",
            "ðŸ”µ REALISTIC ACCURACY (56.3%) - No leakage\n",
            "\n",
            "================================================================================\n",
            "âœ… CORRECT METHOD\n",
            "================================================================================\n",
            "Train: 1900 | Test: 476\n",
            "Class dist: UP=51.4%\n",
            "\n",
            "Training SVM...\n",
            "\n",
            "                                    RESULTS                                     \n",
            "--------------------------------------------------------------------------------\n",
            "Test Accuracy: 0.5630 (56.30%)\n",
            "\n",
            "Confusion Matrix:\n",
            "                Predicted DOWN  Predicted UP\n",
            "Actual DOWN          0             208     \n",
            "Actual UP            0             268     \n",
            "\n",
            "Class-wise Accuracy:\n",
            "DOWN: 0.0000 (0.0%)\n",
            "UP:   1.0000 (100.0%)\n",
            "Balance: 1.0000\n",
            "\n",
            "ðŸ”µ REALISTIC ACCURACY (56.3%) - No leakage\n",
            "\n",
            "================================================================================\n",
            "ðŸ“Š SUMMARY - ACCURACY COMPARISON\n",
            "================================================================================\n",
            "\n",
            "Scenario                                      Accuracy     Status\n",
            "--------------------------------------------------------------------------------\n",
            "Scenario 1: EXTREME LEAKAGE (NO LAG + SAME DAY) 78.78%       ðŸŸ¡ HIGH (Leakage)\n",
            "Scenario 2: NORMALIZE BEFORE SPLIT + SHUFFLE  52.31%       ðŸ”µ REALISTIC\n",
            "Scenario 3: CORRECT BUT RANDOM CV             56.30%       ðŸ”µ REALISTIC\n",
            "Scenario 4: âœ… FULLY CORRECT                   56.30%       ðŸ”µ REALISTIC\n",
            "\n",
            "================================================================================\n",
            "ðŸ’¡ CONCLUSIONS\n",
            "================================================================================\n",
            "\n",
            "1. If Scenario 1-2 achieves 85-90%, the paper has SEVERE data leakage\n",
            "2. If Scenario 3 is high but 4 is low, paper used random CV incorrectly\n",
            "3. If ALL scenarios show ~55-60%, your implementation is CORRECT\n",
            "   and the paper's methodology is QUESTIONABLE\n",
            "\n",
            "Your realistic accuracy (~56%) is NORMAL for financial prediction!\n",
            "Papers reporting 85-90% are almost always using incorrect methodology.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "âœ… ANALYSIS COMPLETE\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZNiMmElcdQa9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}